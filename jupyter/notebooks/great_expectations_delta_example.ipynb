{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df68cc7d-3bf9-4072-8cbe-8ac5434b5c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import great_expectations as gx\n",
    "from great_expectations.core.batch import BatchRequest, RuntimeBatchRequest\n",
    "from ruamel import yaml\n",
    "import pyspark\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f89b957-44e8-42e1-bcb1-2fe11e34da55",
   "metadata": {},
   "source": [
    "## Get Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89dde78-9406-4018-904c-2bbd30ecc4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
       "        'HOSTNAME': '2549534bc938',\n",
       "        'S3_ENDPOINT': 'http://minio:9000',\n",
       "        'S3_BUCKET': 'test',\n",
       "        'S3_ACCESS_KEY': 'sparkaccesskey',\n",
       "        'S3_SECRET_KEY': 'sparksupersecretkey',\n",
       "        'LANG': 'C.UTF-8',\n",
       "        'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D',\n",
       "        'PYTHON_VERSION': '3.11.0',\n",
       "        'PYTHON_PIP_VERSION': '22.3',\n",
       "        'PYTHON_SETUPTOOLS_VERSION': '65.5.0',\n",
       "        'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/66030fa03382b4914d4c4d0896961a0bdeeeb274/public/get-pip.py',\n",
       "        'PYTHON_GET_PIP_SHA256': '1e501cf004eac1b7eb1f97266d28f995ae835d30250bec7f8850562703067dc6',\n",
       "        'HOME': '/root',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'JPY_SESSION_NAME': '/notebooks/great_expectations_trino_example.ipynb',\n",
       "        'JPY_PARENT_PID': '1',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'FORCE_COLOR': '1',\n",
       "        'CLICOLOR_FORCE': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ \n",
    "## Should see S3_ENDPOINT, S3_ACCESS_KEY, and S3_SECRET_KEY environment varibles.\n",
    "# These environment variables are set in the docker-compose.yml, and the service account used by PySpark\n",
    "#> to read from and write to Minio are created by the minio-init container defined in docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476cfaea-2226-48b4-853f-efceb0f791aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S3_ACCESS_KEY = os.environ.get(\"S3_ACCESS_KEY\")\n",
    "S3_BUCKET = os.environ.get(\"S3_BUCKET\")\n",
    "S3_SECRET_KEY = os.environ.get(\"S3_SECRET_KEY\")\n",
    "S3_ENDPOINT = os.environ.get(\"S3_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c46feb19-387b-48f5-9751-85f7b5c4e63f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-986528d1-b281-48c2-ae05-339cebebba37;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 294ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-986528d1-b281-48c2-ae05-339cebebba37\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/15ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/19 03:07:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# This cell may take some time to run the first time, as it must download the necessary spark jars\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "## IF YOU ARE USING THE SPARK CONTAINERS, UNCOMMENT THE LINE BELOW TO OFFLOAD EXECUTION OF SPARK TASKS TO SPARK CONTAINERS\n",
    "#conf.setMaster(\"spark://spark:7077\")\n",
    "\n",
    "conf.set(\"spark.jars.packages\", 'org.apache.hadoop:hadoop-aws:3.3.1,io.delta:delta-core_2.12:2.1.0')\n",
    "# conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\n",
    "conf.set('spark.hadoop.fs.s3a.endpoint', S3_ENDPOINT)\n",
    "conf.set('spark.hadoop.fs.s3a.access.key', S3_ACCESS_KEY)\n",
    "conf.set('spark.hadoop.fs.s3a.secret.key', S3_SECRET_KEY)\n",
    "conf.set('spark.hadoop.fs.s3a.path.style.access', \"true\")\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82334983-1c2a-4c99-8c38-4c107b1eab47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f935e04e-9802-4a52-8e8d-25bcab8be52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/19 03:07:29 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "delta_table_name = \"appl_stock_delta_table\"\n",
    "df = spark.read.format(\"delta\").load(f\"s3a://{S3_BUCKET}/{delta_table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a99a19-7e1e-453c-bccc-8c90ba39885b",
   "metadata": {},
   "source": [
    "## Prepare Great Expectations Context and Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb0cbe41-a414-42d1-9b2a-9f4afc16ad12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/19 03:07:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fcdbbd8-cb06-441a-a5ee-485c084cb050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasource_name = \"delta_lake\"\n",
    "\n",
    "config = f\"\"\"\n",
    "name: {datasource_name}\n",
    "class_name: Datasource\n",
    "execution_engine:\n",
    "  class_name: SparkDFExecutionEngine\n",
    "data_connectors:\n",
    "  default_runtime_data_connector_name:\n",
    "    class_name: RuntimeDataConnector\n",
    "    batch_identifiers:\n",
    "      - batch_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a55c25ab-132b-4a92-a96d-6b3571d39693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to instantiate class from config...\n",
      "\tInstantiating as a Datasource, since class_name is Datasource\n",
      "\tSuccessfully instantiated Datasource\n",
      "\n",
      "\n",
      "ExecutionEngine class name: SparkDFExecutionEngine\n",
      "Data Connectors:\n",
      "\tdefault_runtime_data_connector_name:RuntimeDataConnector\n",
      "\n",
      "\tAvailable data_asset_names (0 of 0):\n",
      "\t\tNote : RuntimeDataConnector will not have data_asset_names until they are passed in through RuntimeBatchRequest\n",
      "\n",
      "\tUnmatched data_references (0 of 0): []\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<great_expectations.datasource.new_datasource.Datasource at 0x7f0fa8dc85d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.test_yaml_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3016935-af03-4e1d-8e3b-0216a8a37980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<great_expectations.datasource.new_datasource.Datasource at 0x7f0f6e895910>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.add_datasource(**yaml.load(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24887e96-b715-4ed5-a6ff-3af6f9a1fc3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_request = RuntimeBatchRequest(\n",
    "    datasource_name=datasource_name,\n",
    "    data_connector_name=\"default_runtime_data_connector_name\",\n",
    "    data_asset_name=\"APPL_TABLE\",  # this is the name of the table you want to retrieve\n",
    "    batch_identifiers={\"batch_id\":\"default_identifier\"},\n",
    "    runtime_parameters={\"batch_data\":df}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "534bd519-5597-4799-b63f-0b400a371963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context.create_expectation_suite(\n",
    "    expectation_suite_name=\"test_suite\", overwrite_existing=True\n",
    ")\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request, expectation_suite_name=\"test_suite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79872462-37ff-4629-a1f9-746f0fe19596",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b109aa8bbd4dd88e48477eeb5d0a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": true,\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"result\": {}\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validator.expect_column_to_exist(\"Close\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
