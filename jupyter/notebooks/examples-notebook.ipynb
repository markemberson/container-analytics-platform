{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Usage with Delta Lake & Minio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to write a CSV file directly to Minio, and also how to write and read a managed Delta Lake table in Minio.\n",
    "\n",
    "Click the Table of Contents button in the left JupyterLab sidebar (the button on the far left of this browser window that looks like a bulleted list) to see the types of examples provided. **Make sure to run all the cells above a given section, since most examples in this notebook depend on those above them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Environment Variables for Minio (S3) Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
       "        'HOSTNAME': '7f4b53a5c2d3',\n",
       "        'S3_ENDPOINT': 'http://minio:9000',\n",
       "        'S3_BUCKET': 'test',\n",
       "        'S3_ACCESS_KEY': 'sparkaccesskey',\n",
       "        'S3_SECRET_KEY': 'sparksupersecretkey',\n",
       "        'LANG': 'C.UTF-8',\n",
       "        'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D',\n",
       "        'PYTHON_VERSION': '3.10.5',\n",
       "        'PYTHON_PIP_VERSION': '22.0.4',\n",
       "        'PYTHON_SETUPTOOLS_VERSION': '58.1.0',\n",
       "        'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/6ce3639da143c5d79b44f94b04080abf2531fd6e/public/get-pip.py',\n",
       "        'PYTHON_GET_PIP_SHA256': 'ba3ab8267d91fd41c58dbce08f76db99f747f716d85ce1865813842bb035524d',\n",
       "        'HOME': '/root',\n",
       "        'JPY_PARENT_PID': '1',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ \n",
    "## Should see S3_ENDPOINT, S3_ACCESS_KEY, and S3_SECRET_KEY environment varibles.\n",
    "# These environment variables are set in the docker-compose.yml, and the service account used by PySpark\n",
    "#> to read from and write to Minio are created by the minio-init container defined in docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S3_ACCESS_KEY = os.environ.get(\"S3_ACCESS_KEY\")\n",
    "S3_BUCKET = os.environ.get(\"S3_BUCKET\")\n",
    "S3_SECRET_KEY = os.environ.get(\"S3_SECRET_KEY\")\n",
    "S3_ENDPOINT = os.environ.get(\"S3_ENDPOINT\")\n",
    "# S3_ACCESS_KEY = \"sparkaccesskey\"\n",
    "# S3_BUCKET = \"test\"\n",
    "# S3_SECRET_KEY = \"sparksupersecretkey\"\n",
    "# S3_ENDPOINT = \"http://minio:9000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Pyspark to Connect to Minio and Enable Delta-Lake Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-182bbaae-596d-4706-a4db-1d0325fd2d2a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.1!hadoop-aws.jar (100ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.1.0/delta-core_2.12-2.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.1.0!delta-core_2.12.jar (285ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.901!aws-java-sdk-bundle.jar (17123ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (57ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.1.0/delta-storage-2.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.1.0!delta-storage.jar (44ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (56ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (37ms)\n",
      ":: resolution report :: resolve 3817ms :: artifacts dl 17719ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   7   |   7   |   0   ||   7   |   7   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-182bbaae-596d-4706-a4db-1d0325fd2d2a\n",
      "\tconfs: [default]\n",
      "\t7 artifacts copied, 0 already retrieved (192837kB/282ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/17 03:24:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# This cell may take some time to run the first time, as it must download the necessary spark jars\n",
    "conf = pyspark.SparkConf().setMaster(\"spark://spark:7077\")\n",
    "conf.set(\"spark.jars.packages\", 'org.apache.hadoop:hadoop-aws:3.3.1,io.delta:delta-core_2.12:2.1.0')\n",
    "# conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\n",
    "conf.set('spark.hadoop.fs.s3a.endpoint', S3_ENDPOINT)\n",
    "conf.set('spark.hadoop.fs.s3a.access.key', S3_ACCESS_KEY)\n",
    "conf.set('spark.hadoop.fs.s3a.secret.key', S3_SECRET_KEY)\n",
    "conf.set('spark.hadoop.fs.s3a.path.style.access', \"true\")\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "# sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Sample CSV Data from Local Filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/data/appl_stock.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"]:\n",
    "    df = df.withColumn(col,df[col].cast('double'))\n",
    "for col in [\"Volume\"]:\n",
    "    df = df.withColumn(col, df[col].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Write CSV Directly to Minio (Not as a Delta Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/17 03:24:20 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "22/09/17 03:24:21 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.csv(f\"s3a://{S3_BUCKET}/appl_stock.csv\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Navigate to http://localhost:9090 and login to the Minio Console to see the CSV file**\n",
    "\n",
    "(username and password for minio can be found in the environment variables section of the minio service definition in the docker-compose.yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a Delta Lake Table in Minio using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to replace spaces in column names with underscores for Delta\n",
    "delta_df = df\n",
    "for col in delta_df.columns:\n",
    "    delta_df = delta_df.withColumnRenamed(col, col.replace(\" \",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj_Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj_Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Month and Year columns for partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df = delta_df.withColumn(\"Month\", month(delta_df.Date))\n",
    "delta_df = delta_df.withColumn(\"Year\", year(delta_df.Date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table_name = \"appl_stock_delta_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "delta_df.write.format(\"delta\").partitionBy('Year','Month').option(\"overwriteSchema\", \"true\").save(f\"s3a://{S3_BUCKET}/{delta_table_name}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Navigate to http://localhost:9090 and login to the Minio Console to see the Delta Lake Table**\n",
    "\n",
    "**Note that the Delta Lake Table includes both the data partitions and the metadata log**\n",
    "\n",
    "(username and password for minio can be found in the environment variables section of the minio service definition in the docker-compose.yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Delta Table Back into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_delta_df = spark.read.format(\"delta\").load(f\"s3a://{S3_BUCKET}/{delta_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+-----+----+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj_Close|Month|Year|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+-----+----+\n",
      "|2011-08-01|397.77999900000003|        399.500011|        392.369995|        396.749989|153209000|          51.40275|    8|2011|\n",
      "|2011-08-02|        397.650009|397.90000200000003|         388.35001|        388.909996|159884900|         50.387004|    8|2011|\n",
      "|2011-08-03|390.98000299999995|        393.549995|         382.23999|            392.57|183127000|         50.861193|    8|2011|\n",
      "|2011-08-04|        389.410007|391.32001099999997|377.34999799999997|        377.369999|217851900|         48.891888|    8|2011|\n",
      "|2011-08-05|        380.440002|        383.499992|        362.570007|        373.620007|301147700|48.406040000000004|    8|2011|\n",
      "|2011-08-08|361.68998700000003|        367.769993|        353.019989|353.21000699999996|285958400|          45.76173|    8|2011|\n",
      "|2011-08-09|        361.299992|374.60998900000004|        355.000008|        374.010002|270645900|         48.456568|    8|2011|\n",
      "|2011-08-10|         371.14999|         374.64999|        362.499992|        363.690006|219664200|         47.119514|    8|2011|\n",
      "|2011-08-11|        370.519989|        375.450008|         364.71999|        373.700008|185492300|         48.416405|    8|2011|\n",
      "|2011-08-12|378.06998799999997|379.64001099999996|        374.230007|         376.98999|132244000|48.842653999999996|    8|2011|\n",
      "|2011-08-15|        379.629997|        384.970013|        378.089989|        383.410004|115136000|49.674428000000006|    8|2011|\n",
      "|2011-08-16|            381.52|        383.370003|        376.060009|380.48000299999995|124687500|         49.294818|    8|2011|\n",
      "|2011-08-17|        382.310005|        384.519989|             378.0|        380.440002|110515300|         49.289636|    8|2011|\n",
      "|2011-08-18|        370.839996|        372.649998|        361.370007|        366.050007|212858800|         47.425275|    8|2011|\n",
      "|2011-08-19|        362.169998|        366.999989|        356.000004|        356.029991|193972100|         46.127086|    8|2011|\n",
      "|2011-08-22|        364.509998|        364.879993|        355.089996|356.43998700000003|133828800|         46.180205|    8|2011|\n",
      "|2011-08-23|        360.299995|373.64000699999997|             357.0|        373.600006|164208800|         48.403449|    8|2011|\n",
      "|2011-08-24|         373.46999|        378.959995|        370.599991|376.18001200000003|156566900|         48.737713|    8|2011|\n",
      "|2011-08-25|        365.079998|        375.450008|        364.999996|        373.720009|217836500|         48.418997|    8|2011|\n",
      "|2011-08-26|371.16999100000004|        383.799999|        370.799995|        383.579994|160369300|         49.696452|    8|2011|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_delta_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Data From Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, f\"s3a://{S3_BUCKET}/{delta_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "delta_table.delete(\"Date < '2010-02-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_table.vacuum()\n",
    "\n",
    "# .vacuum() is not really necessary for this example. For more info, see https://docs.delta.io/latest/delta-utility.html#remove-files-no-longer-referenced-by-a-delta-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = delta_table.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+\n",
      "|summary|      Date|              Open|              High|               Low|             Close|             Volume|         Adj_Close|            Month|              Year|\n",
      "+-------+----------+------------------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+\n",
      "|  count|      1743|              1743|              1743|              1743|              1743|               1743|              1743|             1743|              1743|\n",
      "|   mean|      null| 314.2063570154905| 317.0524208313255|310.96767053872634| 314.0739527596099|9.307720510613884E7| 75.52596069420544|6.610441767068273|2013.0338496844522|\n",
      "| stddev|      null|185.98853126511324|187.59247321328306|184.05333687045757|185.82494613123123|5.851694048954227E7|  28.2829791099516|3.395013054378821|1.9874755475272696|\n",
      "|    min|2010-02-01|              90.0|         90.699997|         89.470001|         90.279999|           11475900|         24.881912|                1|              2010|\n",
      "|    max|2016-12-30|        702.409988|        705.070023|        699.569977|        702.100021|          470249500|127.96609099999999|               12|              2016|\n",
      "+-------+----------+------------------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "updated_df.describe().show()\n",
    "# Notice the min date due to the delete above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Time Travel to Read a Previous Version of the Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+\n",
      "|summary|      Date|              Open|              High|               Low|            Close|             Volume|         Adj_Close|             Month|              Year|\n",
      "+-------+----------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+\n",
      "|  count|      1762|              1762|              1762|              1762|             1762|               1762|              1762|              1762|              1762|\n",
      "|   mean|      null|313.07631115891036|315.91128801645874| 309.8282405079455|312.9270656379115|9.422577587968218E7| 75.00174115607268|6.5499432463110105|2013.0011350737798|\n",
      "| stddev|      null|185.29946803981548|186.89817686485782|183.38391664370968|185.1471036170944|6.020518776592716E7|28.574929721799016|3.4260339585331754| 2.001418822382117|\n",
      "|    min|2010-01-04|              90.0|         90.699997|         89.470001|        90.279999|           11475900|         24.881912|                 1|              2010|\n",
      "|    max|2016-12-30|        702.409988|        705.070023|        699.569977|       702.100021|          470249500|127.96609099999999|                12|              2016|\n",
      "+-------+----------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "previous_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(f\"s3a://{S3_BUCKET}/{delta_table_name}\")\n",
    "previous_df.describe().show()\n",
    "# Notice the min date, showing that we are reading from a previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      1|2022-09-17 03:24:52|  null|    null|   DELETE|{predicate -> [\"(...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      0|2022-09-17 03:24:35|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 84, ...|        null|Apache-Spark/3.3....|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigger Trino to Automatically Infer Schema from Delta Table and Make Data Available for End User Querying / Dashboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table_name = \"appl_stock_delta_table\"\n",
    "delta_schema_name = \"my_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to simplify query execution against Trino REST API\n",
    "def execute_trino_query(query, statement_endpoint = \"http://trino:8080/v1/statement\", user = \"admin\", password = \"\"):\n",
    "    \n",
    "    print(f\"Executing query:\\n{query}\")\n",
    "    res = requests.post(statement_endpoint,data = query.encode(\"UTF8\"), auth=requests.auth.HTTPBasicAuth(user,password))\n",
    "    \n",
    "    data = []\n",
    "    cols = None\n",
    "    while True:\n",
    "        json_res = res.json()\n",
    "        state = json_res.get(\"stats\").get(\"state\")\n",
    "        print(f\"State: {state}\")\n",
    "\n",
    "        res_data = json_res.get(\"data\")\n",
    "        if res_data:\n",
    "            data.extend(res_data)\n",
    "        \n",
    "        res_cols = json_res.get(\"columns\")\n",
    "        if res_cols:\n",
    "            cols = [i[\"name\"] for i in res_cols]\n",
    "            \n",
    "        next_uri = json_res.get(\"nextUri\")\n",
    "        if next_uri:\n",
    "            sleep(.5)\n",
    "            res = requests.get(next_uri)\n",
    "        else:\n",
    "            return [dict(zip(cols, d)) for d in data]\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigger Trino to Read Delta Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_schema_statement = f\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS delta.my_schema\n",
    "WITH (location = 's3a://{S3_BUCKET}/')\n",
    "\"\"\"\n",
    "\n",
    "create_table_statement = f\"\"\"CREATE TABLE IF NOT EXISTS delta.{delta_schema_name}.{delta_table_name} (\n",
    "  dummy bigint\n",
    ")\n",
    "WITH (\n",
    "  location = 's3a://{S3_BUCKET}/{delta_table_name}'\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query:\n",
      "\n",
      "CREATE SCHEMA IF NOT EXISTS delta.my_schema\n",
      "WITH (location = 's3a://test/')\n",
      "\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: FINISHED\n",
      "[{'result': True}]\n",
      "Executing query:\n",
      "CREATE TABLE IF NOT EXISTS delta.my_schema.appl_stock_delta_table (\n",
      "  dummy bigint\n",
      ")\n",
      "WITH (\n",
      "  location = 's3a://test/appl_stock_delta_table'\n",
      ")\n",
      "\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: FINISHED\n",
      "[{'result': True}]\n"
     ]
    }
   ],
   "source": [
    "for query in [create_schema_statement, create_table_statement]:\n",
    "    print(execute_trino_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Data from Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 10\n",
    "select_statement = f\"SELECT * FROM delta.{delta_schema_name}.{delta_table_name}\"\n",
    "if LIMIT and type(LIMIT) == int:\n",
    "    select_statement += f\" LIMIT {LIMIT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query:\n",
      "SELECT * FROM delta.my_schema.appl_stock_delta_table LIMIT 10\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: RUNNING\n",
      "State: FINISHING\n",
      "State: FINISHED\n"
     ]
    }
   ],
   "source": [
    "data = execute_trino_query(select_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'date': '2015-06-01', 'open': 130.279999, 'high': 131.389999, 'low': 130.050003, 'close': 130.53999299999998, 'volume': 32112800, 'adj_close': 126.03511, 'month': 6, 'year': 2015}, {'date': '2013-08-01', 'open': 455.74997699999994, 'high': 456.799988, 'low': 453.259987, 'close': 456.679985, 'volume': 51562700, 'adj_close': 60.437984, 'month': 8, 'year': 2013}, {'date': '2013-05-01', 'open': 444.45999900000004, 'high': 444.929996, 'low': 434.389996, 'close': 439.29000099999996, 'volume': 126727300, 'adj_close': 57.754282999999994, 'month': 5, 'year': 2013}, {'date': '2013-09-03', 'open': 493.099991, 'high': 500.59997599999997, 'low': 487.35000599999995, 'close': 488.57999400000006, 'volume': 82982200, 'adj_close': 65.086626, 'month': 9, 'year': 2013}, {'date': '2011-07-01', 'open': 335.950012, 'high': 343.500011, 'low': 334.200012, 'close': 343.26000600000003, 'volume': 108828300, 'adj_close': 44.472612, 'month': 7, 'year': 2011}, {'date': '2010-09-01', 'open': 247.46999, 'high': 251.45998799999998, 'low': 246.280003, 'close': 250.330002, 'volume': 174259400, 'adj_close': 32.432643, 'month': 9, 'year': 2010}, {'date': '2013-08-02', 'open': 458.01000199999993, 'high': 462.85000599999995, 'low': 456.66001100000005, 'close': 462.53998600000006, 'volume': 68695900, 'adj_close': 61.213508999999995, 'month': 8, 'year': 2013}, {'date': '2013-08-05', 'open': 464.689995, 'high': 470.669998, 'low': 462.150017, 'close': 469.449997, 'volume': 79713900, 'adj_close': 62.127995, 'month': 8, 'year': 2013}, {'date': '2013-05-02', 'open': 441.779991, 'high': 448.58997300000004, 'low': 440.630005, 'close': 445.519997, 'volume': 105457100, 'adj_close': 58.573352, 'month': 5, 'year': 2013}, {'date': '2013-05-03', 'open': 451.30998200000005, 'high': 453.23002599999995, 'low': 449.149986, 'close': 449.980019, 'volume': 90325200, 'adj_close': 59.159718999999996, 'month': 5, 'year': 2013}]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d04c0d231515a3f3089cb8e547ddeb6d0ef1c95d45851d9802a6754ba01b59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
