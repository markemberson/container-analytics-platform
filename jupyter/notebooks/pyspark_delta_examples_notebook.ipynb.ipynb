{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Usage with Delta Lake & Minio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to write a CSV file directly to Minio, and also how to write and read a managed Delta Lake table in Minio.\n",
    "\n",
    "Click the Table of Contents button in the left JupyterLab sidebar (the button on the far left of this browser window that looks like a bulleted list) to see the types of examples provided. **Make sure to run all the cells above a given section, since most examples in this notebook depend on those above them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Environment Variables for Minio (S3) Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
       "        'HOSTNAME': '698deab91192',\n",
       "        'S3_ENDPOINT': 'http://minio:9000',\n",
       "        'S3_BUCKET': 'test',\n",
       "        'S3_ACCESS_KEY': 'sparkaccesskey',\n",
       "        'S3_SECRET_KEY': 'sparksupersecretkey',\n",
       "        'LANG': 'C.UTF-8',\n",
       "        'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D',\n",
       "        'PYTHON_VERSION': '3.10.7',\n",
       "        'PYTHON_PIP_VERSION': '22.2.2',\n",
       "        'PYTHON_SETUPTOOLS_VERSION': '63.2.0',\n",
       "        'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/5eaac1050023df1f5c98b173b248c260023f2278/public/get-pip.py',\n",
       "        'PYTHON_GET_PIP_SHA256': '5aefe6ade911d997af080b315ebcb7f882212d070465df544e1175ac2be519b4',\n",
       "        'HOME': '/root',\n",
       "        'JPY_PARENT_PID': '1',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ \n",
    "## Should see S3_ENDPOINT, S3_ACCESS_KEY, and S3_SECRET_KEY environment varibles.\n",
    "# These environment variables are set in the docker-compose.yml, and the service account used by PySpark\n",
    "#> to read from and write to Minio are created by the minio-init container defined in docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S3_ACCESS_KEY = os.environ.get(\"S3_ACCESS_KEY\")\n",
    "S3_BUCKET = os.environ.get(\"S3_BUCKET\")\n",
    "S3_SECRET_KEY = os.environ.get(\"S3_SECRET_KEY\")\n",
    "S3_ENDPOINT = os.environ.get(\"S3_ENDPOINT\")\n",
    "# S3_ACCESS_KEY = \"sparkaccesskey\"\n",
    "# S3_BUCKET = \"test\"\n",
    "# S3_SECRET_KEY = \"sparksupersecretkey\"\n",
    "# S3_ENDPOINT = \"http://minio:9000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Pyspark to Connect to Minio and Enable Delta-Lake Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-40eba771-bfce-4f89-8e25-e5fdf8942688;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.1!hadoop-aws.jar (286ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.1.0/delta-core_2.12-2.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.1.0!delta-core_2.12.jar (584ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.901!aws-java-sdk-bundle.jar (22048ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (166ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.1.0/delta-storage-2.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.1.0!delta-storage.jar (76ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (236ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (58ms)\n",
      ":: resolution report :: resolve 7958ms :: artifacts dl 23660ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   7   |   7   |   0   ||   7   |   7   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-40eba771-bfce-4f89-8e25-e5fdf8942688\n",
      "\tconfs: [default]\n",
      "\t7 artifacts copied, 0 already retrieved (192837kB/14985ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/08 03:00:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# This cell may take some time to run the first time, as it must download the necessary spark jars\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "## IF YOU ARE USING THE SPARK CONTAINERS, UNCOMMENT THE LINE BELOW TO OFFLOAD EXECUTION OF SPARK TASKS TO SPARK CONTAINERS\n",
    "#conf.setMaster(\"spark://spark:7077\")\n",
    "\n",
    "conf.set(\"spark.jars.packages\", 'org.apache.hadoop:hadoop-aws:3.3.1,io.delta:delta-core_2.12:2.1.0')\n",
    "# conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\n",
    "conf.set('spark.hadoop.fs.s3a.endpoint', S3_ENDPOINT)\n",
    "conf.set('spark.hadoop.fs.s3a.access.key', S3_ACCESS_KEY)\n",
    "conf.set('spark.hadoop.fs.s3a.secret.key', S3_SECRET_KEY)\n",
    "conf.set('spark.hadoop.fs.s3a.path.style.access', \"true\")\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "# sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Sample CSV Data from Local Filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/data/appl_stock.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"]:\n",
    "    df = df.withColumn(col,df[col].cast('double'))\n",
    "for col in [\"Volume\"]:\n",
    "    df = df.withColumn(col, df[col].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Write CSV Directly to Minio (Not as a Delta Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/08 03:02:41 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/02/08 03:02:45 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "23/02/08 03:02:47 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.csv(f\"s3a://{S3_BUCKET}/appl_stock.csv\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Navigate to http://localhost:9090 and login to the Minio Console to see the CSV file**\n",
    "\n",
    "(username and password for minio can be found in the environment variables section of the minio service definition in the docker-compose.yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a Delta Lake Table in Minio using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to replace spaces in column names with underscores for Delta\n",
    "delta_df = df\n",
    "for col in delta_df.columns:\n",
    "    delta_df = delta_df.withColumnRenamed(col, col.replace(\" \",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj_Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj_Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Month and Year columns for partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df = delta_df.withColumn(\"Month\", month(delta_df.Date))\n",
    "delta_df = delta_df.withColumn(\"Year\", year(delta_df.Date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table_name = \"appl_stock_delta_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "delta_df.write.format(\"delta\").partitionBy('Year','Month').option(\"overwriteSchema\", \"true\").save(f\"s3a://{S3_BUCKET}/{delta_table_name}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Navigate to http://localhost:9090 and login to the Minio Console to see the Delta Lake Table**\n",
    "\n",
    "**Note that the Delta Lake Table includes both the data partitions and the metadata log**\n",
    "\n",
    "(username and password for minio can be found in the environment variables section of the minio service definition in the docker-compose.yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Delta Table Back into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_delta_df = spark.read.format(\"delta\").load(f\"s3a://{S3_BUCKET}/{delta_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+-----+----+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj_Close|Month|Year|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+-----+----+\n",
      "|2011-08-01|397.77999900000003|        399.500011|        392.369995|        396.749989|153209000|          51.40275|    8|2011|\n",
      "|2011-08-02|        397.650009|397.90000200000003|         388.35001|        388.909996|159884900|         50.387004|    8|2011|\n",
      "|2011-08-03|390.98000299999995|        393.549995|         382.23999|            392.57|183127000|         50.861193|    8|2011|\n",
      "|2011-08-04|        389.410007|391.32001099999997|377.34999799999997|        377.369999|217851900|         48.891888|    8|2011|\n",
      "|2011-08-05|        380.440002|        383.499992|        362.570007|        373.620007|301147700|48.406040000000004|    8|2011|\n",
      "|2011-08-08|361.68998700000003|        367.769993|        353.019989|353.21000699999996|285958400|          45.76173|    8|2011|\n",
      "|2011-08-09|        361.299992|374.60998900000004|        355.000008|        374.010002|270645900|         48.456568|    8|2011|\n",
      "|2011-08-10|         371.14999|         374.64999|        362.499992|        363.690006|219664200|         47.119514|    8|2011|\n",
      "|2011-08-11|        370.519989|        375.450008|         364.71999|        373.700008|185492300|         48.416405|    8|2011|\n",
      "|2011-08-12|378.06998799999997|379.64001099999996|        374.230007|         376.98999|132244000|48.842653999999996|    8|2011|\n",
      "|2011-08-15|        379.629997|        384.970013|        378.089989|        383.410004|115136000|49.674428000000006|    8|2011|\n",
      "|2011-08-16|            381.52|        383.370003|        376.060009|380.48000299999995|124687500|         49.294818|    8|2011|\n",
      "|2011-08-17|        382.310005|        384.519989|             378.0|        380.440002|110515300|         49.289636|    8|2011|\n",
      "|2011-08-18|        370.839996|        372.649998|        361.370007|        366.050007|212858800|         47.425275|    8|2011|\n",
      "|2011-08-19|        362.169998|        366.999989|        356.000004|        356.029991|193972100|         46.127086|    8|2011|\n",
      "|2011-08-22|        364.509998|        364.879993|        355.089996|356.43998700000003|133828800|         46.180205|    8|2011|\n",
      "|2011-08-23|        360.299995|373.64000699999997|             357.0|        373.600006|164208800|         48.403449|    8|2011|\n",
      "|2011-08-24|         373.46999|        378.959995|        370.599991|376.18001200000003|156566900|         48.737713|    8|2011|\n",
      "|2011-08-25|        365.079998|        375.450008|        364.999996|        373.720009|217836500|         48.418997|    8|2011|\n",
      "|2011-08-26|371.16999100000004|        383.799999|        370.799995|        383.579994|160369300|         49.696452|    8|2011|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_delta_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Data From Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, f\"s3a://{S3_BUCKET}/{delta_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "delta_table.delete(\"Date < '2010-02-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_table.vacuum()\n",
    "\n",
    "# .vacuum() is not really necessary for this example. For more info, see https://docs.delta.io/latest/delta-utility.html#remove-files-no-longer-referenced-by-a-delta-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = delta_table.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+\n",
      "|summary|      Date|              Open|              High|               Low|             Close|             Volume|         Adj_Close|            Month|              Year|\n",
      "+-------+----------+------------------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+\n",
      "|  count|      1743|              1743|              1743|              1743|              1743|               1743|              1743|             1743|              1743|\n",
      "|   mean|      null| 314.2063570154904| 317.0524208313254|310.96767053872634| 314.0739527596097|9.307720510613884E7| 75.52596069420544|6.610441767068273|2013.0338496844522|\n",
      "| stddev|      null|185.98853126511304|187.59247321328297|184.05333687045763|185.82494613123114|5.851694048954224E7|28.282979109951594|3.395013054378822|1.9874755475272932|\n",
      "|    min|2010-02-01|              90.0|         90.699997|         89.470001|         90.279999|           11475900|         24.881912|                1|              2010|\n",
      "|    max|2016-12-30|        702.409988|        705.070023|        699.569977|        702.100021|          470249500|127.96609099999999|               12|              2016|\n",
      "+-------+----------+------------------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "updated_df.describe().show()\n",
    "# Notice the min date due to the delete above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Time Travel to READ a Previous Version of the Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+\n",
      "|summary|      Date|              Open|              High|               Low|            Close|             Volume|         Adj_Close|             Month|              Year|\n",
      "+-------+----------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+\n",
      "|  count|      1762|              1762|              1762|              1762|             1762|               1762|              1762|              1762|              1762|\n",
      "|   mean|      null|313.07631115891024| 315.9112880164587| 309.8282405079455|312.9270656379113|9.422577587968218E7| 75.00174115607268|6.5499432463110105|2013.0011350737798|\n",
      "| stddev|      null|185.29946803981525|186.89817686485776|183.38391664370974|185.1471036170943|6.020518776592713E7|28.574929721799023|3.4260339585331767|2.0014188223819636|\n",
      "|    min|2010-01-04|              90.0|         90.699997|         89.470001|        90.279999|           11475900|         24.881912|                 1|              2010|\n",
      "|    max|2016-12-30|        702.409988|        705.070023|        699.569977|       702.100021|          470249500|127.96609099999999|                12|              2016|\n",
      "+-------+----------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "previous_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(f\"s3a://{S3_BUCKET}/{delta_table_name}\")\n",
    "previous_df.describe().show()\n",
    "# Notice the min date, showing that we are reading from a previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      1|2023-02-08 03:04:55|  null|    null|   DELETE|{predicate -> [\"(...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      0|2023-02-08 03:03:35|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 84, ...|        null|Apache-Spark/3.3....|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Time Travel to RESTORE a Previous Version of the Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture a timestamp before we restore the delta table so we can see how to use a timestamp to do restore later on\n",
    "pre_restore_time = datetime.now().strftime(\"%Y-%m-%d %X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
      "|table_size_after_restore|num_of_files_after_restore|num_removed_files|num_restored_files|removed_files_size|restored_files_size|\n",
      "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
      "|                  251203|                        84|                0|                 0|                 0|                  0|\n",
      "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Restore to a numbered version, and show the result summary of the restore operation\n",
    "delta_table.restoreToVersion(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      3|2023-02-08 03:07:42|  null|    null|  RESTORE|{version -> 0, ti...|null|    null|     null|          2|  Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.3....|\n",
      "|      2|2023-02-08 03:06:26|  null|    null|  RESTORE|{version -> 0, ti...|null|    null|     null|          1|  Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.3....|\n",
      "|      1|2023-02-08 03:04:55|  null|    null|   DELETE|{predicate -> [\"(...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      0|2023-02-08 03:03:35|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 84, ...|        null|Apache-Spark/3.3....|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can always un-restore (or restore a more recent version) because restoring is a metadata-only operation\n",
    "##  (i.e. the data files themselves are not modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
      "|table_size_after_restore|num_of_files_after_restore|num_removed_files|num_restored_files|removed_files_size|restored_files_size|\n",
      "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
      "|                  248284|                        83|                1|                 0|              2919|                  0|\n",
      "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#delta_table.restoreToVersion(1)\n",
    "\n",
    "# Instead of using restoreToVersion, we can use a timestamp to revert to the table as it was at a specific time\n",
    "delta_table.restoreToTimestamp(pre_restore_time).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      4|2023-02-08 03:08:49|  null|    null|  RESTORE|{version -> null,...|null|    null|     null|          3|  Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.3....|\n",
      "|      3|2023-02-08 03:07:42|  null|    null|  RESTORE|{version -> 0, ti...|null|    null|     null|          2|  Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.3....|\n",
      "|      2|2023-02-08 03:06:26|  null|    null|  RESTORE|{version -> 0, ti...|null|    null|     null|          1|  Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.3....|\n",
      "|      1|2023-02-08 03:04:55|  null|    null|   DELETE|{predicate -> [\"(...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      0|2023-02-08 03:03:35|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 84, ...|        null|Apache-Spark/3.3....|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigger Trino to Automatically Infer Schema from Delta Table and Make Data Available for End User Querying / Dashboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table_name = \"appl_stock_delta_table\"\n",
    "delta_schema_name = \"my_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to simplify query execution against Trino REST API\n",
    "def execute_trino_query(query, statement_endpoint = \"http://trino:8080/v1/statement\", user = \"admin\", password = \"\"):\n",
    "    \n",
    "    print(f\"Executing query:\\n{query}\")\n",
    "    res = requests.post(statement_endpoint,data = query.encode(\"UTF8\"), auth=requests.auth.HTTPBasicAuth(user,password))\n",
    "    \n",
    "    data = []\n",
    "    cols = None\n",
    "    while True:\n",
    "        json_res = res.json()\n",
    "        state = json_res.get(\"stats\").get(\"state\")\n",
    "        print(f\"State: {state}\")\n",
    "\n",
    "        res_data = json_res.get(\"data\")\n",
    "        if res_data:\n",
    "            data.extend(res_data)\n",
    "        \n",
    "        res_cols = json_res.get(\"columns\")\n",
    "        if res_cols:\n",
    "            cols = [i[\"name\"] for i in res_cols]\n",
    "            \n",
    "        next_uri = json_res.get(\"nextUri\")\n",
    "        if next_uri:\n",
    "            sleep(.5)\n",
    "            res = requests.get(next_uri)\n",
    "        else:\n",
    "            return [dict(zip(cols, d)) for d in data]\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trigger Trino to Read Delta Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_schema_statement = f\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS delta.my_schema\n",
    "WITH (location = 's3a://{S3_BUCKET}/')\n",
    "\"\"\"\n",
    "\n",
    "create_table_statement = f\"\"\"CREATE TABLE IF NOT EXISTS delta.{delta_schema_name}.{delta_table_name} (\n",
    "  dummy bigint\n",
    ")\n",
    "WITH (\n",
    "  location = 's3a://{S3_BUCKET}/{delta_table_name}'\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query:\n",
      "\n",
      "CREATE SCHEMA IF NOT EXISTS delta.my_schema\n",
      "WITH (location = 's3a://test/')\n",
      "\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: FINISHED\n",
      "[{'result': True}]\n",
      "Executing query:\n",
      "CREATE TABLE IF NOT EXISTS delta.my_schema.appl_stock_delta_table (\n",
      "  dummy bigint\n",
      ")\n",
      "WITH (\n",
      "  location = 's3a://test/appl_stock_delta_table'\n",
      ")\n",
      "\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: FINISHED\n",
      "[{'result': True}]\n"
     ]
    }
   ],
   "source": [
    "for query in [create_schema_statement, create_table_statement]:\n",
    "    print(execute_trino_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Data from Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 10\n",
    "select_statement = f\"SELECT * FROM delta.{delta_schema_name}.{delta_table_name}\"\n",
    "if LIMIT and type(LIMIT) == int:\n",
    "    select_statement += f\" LIMIT {LIMIT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query:\n",
      "SELECT * FROM delta.my_schema.appl_stock_delta_table LIMIT 10\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: QUEUED\n",
      "State: RUNNING\n",
      "State: RUNNING\n",
      "State: RUNNING\n",
      "State: RUNNING\n",
      "State: RUNNING\n",
      "State: FINISHED\n",
      "State: FINISHED\n",
      "State: FINISHED\n",
      "State: FINISHED\n"
     ]
    }
   ],
   "source": [
    "data = execute_trino_query(select_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'date': '2015-02-02', 'open': 118.050003, 'high': 119.16999799999999, 'low': 116.08000200000001, 'close': 118.629997, 'volume': 62739100, 'adj_close': 113.611314, 'month': 2, 'year': 2015}, {'date': '2015-04-01', 'open': 124.82, 'high': 125.120003, 'low': 123.099998, 'close': 124.25, 'volume': 40621400, 'adj_close': 119.463174, 'month': 4, 'year': 2015}, {'date': '2015-08-03', 'open': 121.5, 'high': 122.57, 'low': 117.519997, 'close': 118.440002, 'volume': 69976000, 'adj_close': 114.35268500000001, 'month': 8, 'year': 2015}, {'date': '2015-08-04', 'open': 117.41999799999999, 'high': 117.699997, 'low': 113.25, 'close': 114.639999, 'volume': 124138600, 'adj_close': 110.683819, 'month': 8, 'year': 2015}, {'date': '2015-08-05', 'open': 112.949997, 'high': 117.440002, 'low': 112.099998, 'close': 115.400002, 'volume': 99312600, 'adj_close': 111.417594, 'month': 8, 'year': 2015}, {'date': '2012-06-01', 'open': 569.159996, 'high': 572.650009, 'low': 560.5200120000001, 'close': 560.989983, 'volume': 130246900, 'adj_close': 72.68160999999999, 'month': 6, 'year': 2012}, {'date': '2013-11-01', 'open': 524.020004, 'high': 524.799995, 'low': 515.840004, 'close': 520.030006, 'volume': 68722500, 'adj_close': 69.276268, 'month': 11, 'year': 2013}, {'date': '2012-06-04', 'open': 561.500008, 'high': 567.4999849999999, 'low': 548.499977, 'close': 564.289978, 'volume': 139248900, 'adj_close': 73.109156, 'month': 6, 'year': 2012}, {'date': '2012-06-05', 'open': 561.269989, 'high': 566.470001, 'low': 558.3300019999999, 'close': 562.830025, 'volume': 97053600, 'adj_close': 72.920005, 'month': 6, 'year': 2012}, {'date': '2015-07-01', 'open': 126.900002, 'high': 126.940002, 'low': 125.989998, 'close': 126.599998, 'volume': 30238800, 'adj_close': 122.23108300000001, 'month': 7, 'year': 2015}]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Superset API To Add Connection to Trino Delta Lake Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: THE STEPS BELOW WILL ONLY WORK IF YOU ARE ALSO USING THE SUPERSET CONTAINERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPERSET_BASE_URL = \"http://superset:8088\"\n",
    "TOKEN_ENDPOINT = f\"{SUPERSET_BASE_URL}/api/v1/security/login\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"password\": \"admin\",\n",
    "  \"provider\": \"db\",\n",
    "  \"refresh\": True,\n",
    "  \"username\": \"admin\"\n",
    "}\n",
    "headers = {\n",
    "    \"Content-Type\":\"application/json\",\n",
    "    \"Accept\":\"application/json\"\n",
    "}\n",
    "res = requests.post(TOKEN_ENDPOINT, data=json.dumps(data), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_token = res.json()[\"access_token\"]\n",
    "headers[\"Authorization\"] = f\"Bearer {auth_token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Disabled CSRF Token in Superset config.py\n",
    "\n",
    "# CSRF_ENDPOINT = f\"{SUPERSET_BASE_URL}/api/v1/security/csrf_token/\"\n",
    "\n",
    "# res = requests.get(CSRF_ENDPOINT,headers=headers)\n",
    "\n",
    "# csrf_token = res.json()[\"result\"]\n",
    "# headers[\"X-CSRFToken\"] = csrf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_ENDPOINT = f\"{SUPERSET_BASE_URL}/api/v1/database/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"database_name\": \"Delta\",\n",
    "    \"engine\": \"trino\",\n",
    "    \"configuration_method\": \"sqlalchemy_form\",\n",
    "    \"catalog\": [\n",
    "        {\n",
    "            \"name\": \"\",\n",
    "            \"value\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"sqlalchemy_uri\": \"trino://trino@trino:8080/delta\",\n",
    "    \"expose_in_sqllab\": True,\n",
    "    \"allow_ctas\": True,\n",
    "    \"allow_cvas\": True,\n",
    "    \"allow_dml\": True,\n",
    "    \"extra_json\": {\n",
    "        \"allows_virtual_table_explore\": True\n",
    "    },\n",
    "    \"extra\": '{\"allows_virtual_table_explore\":true,\"metadata_params\":{},\"engine_params\":{},\"schemas_allowed_for_file_upload\":[]}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post(DATABASE_ENDPOINT, data=json.dumps(data), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Content-Type': 'application/json',\n",
       " 'Accept': 'application/json',\n",
       " 'Authorization': 'Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcmVzaCI6dHJ1ZSwiaWF0IjoxNjcyMzc3NjM0LCJqdGkiOiIyODQ2MzIxYS1jNThkLTRhODctOGNkNy04YzQ1OWNhMmMwOGMiLCJ0eXBlIjoiYWNjZXNzIiwic3ViIjoxLCJuYmYiOjE2NzIzNzc2MzQsImV4cCI6MTY3MjM3ODUzNH0.lAyBxRjFBsuSZDonwjSVby7RUKTopD4UG4QYzriKj1o'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"id\":1,\"result\":{\"allow_ctas\":true,\"allow_cvas\":true,\"allow_dml\":true,\"configuration_method\":\"sqlalchemy_form\",\"database_name\":\"Delta\",\"expose_in_sqllab\":true,\"extra\":\"{\\\\\"allows_virtual_table_explore\\\\\":true,\\\\\"metadata_params\\\\\":{},\\\\\"engine_params\\\\\":{},\\\\\"schemas_allowed_for_file_upload\\\\\":[]}\",\"sqlalchemy_uri\":\"trino://trino@trino:8080/delta\"}}\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d04c0d231515a3f3089cb8e547ddeb6d0ef1c95d45851d9802a6754ba01b59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
